{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d4a046e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 datetime     DEMAND  Rolling_Mean_Forecast\n",
      "3     2015-01-31 04:00:00   889.0806             915.128025\n",
      "4     2015-01-31 05:00:00   910.1472             904.114375\n",
      "5     2015-01-31 06:00:00   922.1737             906.191300\n",
      "6     2015-01-31 07:00:00   939.9442             915.336425\n",
      "7     2015-01-31 08:00:00  1077.8575             962.530650\n",
      "...                   ...        ...                    ...\n",
      "36715 2019-04-09 20:00:00  1397.5788            1398.827775\n",
      "36716 2019-04-09 21:00:00  1350.5093            1388.684700\n",
      "36717 2019-04-09 22:00:00  1308.7955            1367.451850\n",
      "36718 2019-04-09 23:00:00  1232.4521            1322.333925\n",
      "36719 2019-04-10 00:00:00  1155.5925            1261.837350\n",
      "\n",
      "[36717 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Rolling Mean Demand Forecasting - it calculates a rolling mean of the past 'n' hours of electricity demand to forecast future demand. It uses a rolling window approach to capture short-term demand trends.\n",
    "import pandas as pd\n",
    "\n",
    "# Load the training data\n",
    "train_df = pd.read_excel(\"./data/train_dataframes.xlsx\")\n",
    "\n",
    "# Specify the rolling window size (e.g., 4 hours)\n",
    "window_size = 4\n",
    "\n",
    "# Create a new column to store the rolling mean forecast\n",
    "train_df['Rolling_Mean_Forecast'] = train_df['DEMAND'].rolling(window=window_size).mean()\n",
    "\n",
    "# Drop rows with missing values in the forecast column\n",
    "train_df = train_df.dropna()\n",
    "\n",
    "# You can now use train_df to evaluate the accuracy of the rolling mean forecast.\n",
    "print(train_df[['datetime', 'DEMAND', 'Rolling_Mean_Forecast']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a083e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 datetime   week_X-2   week_X-3   week_X-4       MA_X-4  \\\n",
      "0     2015-01-31 01:00:00   962.2865   906.9580   970.3450   938.004850   \n",
      "1     2015-01-31 02:00:00   933.3221   863.5135   912.1755   900.284075   \n",
      "2     2015-01-31 03:00:00   903.9817   848.4447   900.2688   881.704325   \n",
      "3     2015-01-31 04:00:00   900.9995   839.8821   889.9538   876.458825   \n",
      "4     2015-01-31 05:00:00   904.3481   847.1073   893.6865   879.190775   \n",
      "...                   ...        ...        ...        ...          ...   \n",
      "36715 2019-04-09 20:00:00  1362.1477  1376.6220  1335.4711  1360.112475   \n",
      "36716 2019-04-09 21:00:00  1330.7467  1338.3879  1305.3631  1332.959900   \n",
      "36717 2019-04-09 22:00:00  1275.5550  1281.6932  1240.1208  1279.486450   \n",
      "36718 2019-04-09 23:00:00  1200.3537  1209.2950  1165.0237  1199.699525   \n",
      "36719 2019-04-10 00:00:00  1141.5161  1149.9883  1099.2100  1130.238133   \n",
      "\n",
      "       dayOfWeek  weekend  holiday  Holiday_ID  hourOfDay    T2M_toc  \\\n",
      "0              1        1        0           0          1  25.308496   \n",
      "1              1        1        0           0          2  25.141443   \n",
      "2              1        1        0           0          3  25.006738   \n",
      "3              1        1        0           0          4  24.899713   \n",
      "4              1        1        0           0          5  24.821558   \n",
      "...          ...      ...      ...         ...        ...        ...   \n",
      "36715          4        0        0           0         20  29.044763   \n",
      "36716          4        0        0           0         21  28.539545   \n",
      "36717          4        0        0           0         22  28.163232   \n",
      "36718          4        0        0           0         23  27.830194   \n",
      "36719          5        0        0           0          0  27.522791   \n",
      "\n",
      "          DEMAND  Interaction_Feature  Day_Week_X-3  \n",
      "0       954.2018         24354.024126      906.9580  \n",
      "1       913.8660         23465.064257      863.5135  \n",
      "2       903.3637         22605.633783      848.4447  \n",
      "3       889.0806         22434.629085      839.8821  \n",
      "4       910.1472         22447.328470      847.1073  \n",
      "...          ...                  ...           ...  \n",
      "36715  1397.5788         39563.257368     5506.4880  \n",
      "36716  1350.5093         37978.904899     5353.5516  \n",
      "36717  1308.7955         35923.751932     5126.7728  \n",
      "36718  1232.4521         33406.076450     4837.1800  \n",
      "36719  1155.5925         31417.708504     5749.9415  \n",
      "\n",
      "[36720 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# Custom Feature Engineering - It performs custom feature engineering by creating new features that capture interactions between existing features.\n",
    "import pandas as pd\n",
    "\n",
    "# Load the training data\n",
    "train_df = pd.read_excel(\"./data/train_dataframes.xlsx\")\n",
    "\n",
    "# Define a custom feature engineering function\n",
    "def custom_feature_engineering(df):\n",
    "    # Create a feature that represents the interaction between 'week_X-2' and 'T2M_toc'\n",
    "    df['Interaction_Feature'] = df['week_X-2'] * df['T2M_toc']\n",
    "    \n",
    "    # Create a feature that represents the day of the week multiplied by 'week_X-3'\n",
    "    df['Day_Week_X-3'] = df['dayOfWeek'] * df['week_X-3']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the custom feature engineering function to the training data\n",
    "train_df = custom_feature_engineering(train_df)\n",
    "\n",
    "# You can now use train_df, including the new features, for training and prediction.\n",
    "print(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d35acc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjyua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\sql\\pandas\\conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n",
      "C:\\Users\\tjyua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\sql\\pandas\\conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Exponential Smoothing: 199.11975817446654\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exponential Smoothing Demand Forecasting\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the data into a Pandas DataFrame\n",
    "excel_data_path = \"./data/train_dataframes.xlsx\"\n",
    "train_df = pd.read_excel(excel_data_path)\n",
    "\n",
    "# Convert the Pandas DataFrame to a Spark DataFrame\n",
    "spark_df: DataFrame = spark.createDataFrame(train_df)\n",
    "\n",
    "# Assume you have test data in another Excel sheet or file\n",
    "test_excel_data_path = \"./data/test_dataframes.xlsx\"\n",
    "test_df = pd.read_excel(test_excel_data_path)\n",
    "\n",
    "# Convert the Pandas DataFrame to a Spark DataFrame for test data\n",
    "spark_test_df: DataFrame = spark.createDataFrame(test_df)\n",
    "\n",
    "def exponential_smoothing_forecast(series, alpha):\n",
    "    forecast = [series[0]]\n",
    "    for t in range(1, len(series)):\n",
    "        forecast.append(alpha * series[t] + (1 - alpha) * forecast[t - 1])\n",
    "    return forecast\n",
    "\n",
    "# Define the alpha parameter (smoothing factor)\n",
    "alpha = 0.2\n",
    "\n",
    "# Check if \"DEMAND\" exists and its data type\n",
    "if \"DEMAND\" not in spark_df.columns:\n",
    "    raise ValueError(\"Column 'DEMAND' not found in the DataFrame\")\n",
    "\n",
    "# Apply exponential smoothing to DEMAND data\n",
    "demand_series = spark_df.select(\"DEMAND\").rdd.map(lambda row: row[0]).collect()\n",
    "exponential_forecast = exponential_smoothing_forecast(demand_series, alpha)\n",
    "\n",
    "# Define the test data\n",
    "test_demand_series = spark_test_df.select(\"DEMAND\").rdd.map(lambda row: row[0]).collect()\n",
    "\n",
    "# Ensure the forecast and test data have the same length\n",
    "forecast_start_index = 0\n",
    "forecast_end_index = len(test_demand_series)\n",
    "aligned_forecast = exponential_forecast[forecast_start_index:forecast_end_index]\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(test_demand_series, aligned_forecast))\n",
    "print(f\"RMSE for Exponential Smoothing: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3293fcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjyua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\sql\\pandas\\conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n",
      "C:\\Users\\tjyua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\sql\\pandas\\conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Exponential Smoothing: 360.1011533061308\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exponential Smoothing Demand Forecasting\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the data into a Pandas DataFrame\n",
    "excel_data_path = \"./data/train_dataframes.xlsx\"\n",
    "train_df = pd.read_excel(excel_data_path)\n",
    "\n",
    "# Convert the Pandas DataFrame to a Spark DataFrame\n",
    "spark_df: DataFrame = spark.createDataFrame(train_df)\n",
    "\n",
    "# Assume you have test data in another Excel sheet or file\n",
    "test_excel_data_path = \"./data/test_dataframes.xlsx\"\n",
    "test_df = pd.read_excel(test_excel_data_path)\n",
    "\n",
    "# Convert the Pandas DataFrame to a Spark DataFrame for test data\n",
    "spark_test_df: DataFrame = spark.createDataFrame(test_df)\n",
    "\n",
    "def exponential_smoothing_forecast(series, alpha):\n",
    "    forecast = [series[0] for t in range(1, len(series))]\n",
    "    for t in range(1, len(series)):\n",
    "        forecast.append(alpha * series[t] + (1 - alpha) * forecast[t - 1])\n",
    "    return forecast\n",
    "\n",
    "# Define the alpha parameter (smoothing factor)\n",
    "alpha = 0.2\n",
    "\n",
    "# Check if \"DEMAND\" exists and its data type\n",
    "if \"DEMAND\" not in spark_df.columns:\n",
    "    raise ValueError(\"Column 'DEMAND' not found in the DataFrame\")\n",
    "\n",
    "# Apply exponential smoothing to DEMAND data\n",
    "demand_series = spark_df.select(\"DEMAND\").rdd.map(lambda row: row[0]).collect()\n",
    "exponential_forecast = exponential_smoothing_forecast(demand_series, alpha)\n",
    "\n",
    "# Define the test data\n",
    "test_demand_series = spark_test_df.select(\"DEMAND\").rdd.map(lambda row: row[0]).collect()\n",
    "\n",
    "# Ensure the forecast and test data have the same length\n",
    "forecast_start_index = 0\n",
    "forecast_end_index = len(test_demand_series)\n",
    "aligned_forecast = exponential_forecast[forecast_start_index:forecast_end_index]\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(test_demand_series, aligned_forecast))\n",
    "print(f\"RMSE for Exponential Smoothing: {rmse}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
