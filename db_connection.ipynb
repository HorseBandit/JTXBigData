{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pymongo\n",
    "%pip install pandas pymongo openpyxl\n",
    "%pip install scikit-learn\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MongoDB Connection\n",
    "client = MongoClient('mongodb+srv://JTXBigData:pJRAyKW9QnqE7B1G@jtxbigdatacluster.dzo50pn.mongodb.net/')\n",
    "db = client['JTXBigDataCluster']\n",
    "collection = db['jtx-reduced-data']\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel(\"./data/train_dataframes.xlsx\")\n",
    "\n",
    "# Assume you want to keep the 'datetime' column but perform PCA on all other columns.\n",
    "datetime_column = df[['datetime']]\n",
    "feature_columns = df.drop(['datetime'], axis=1)\n",
    "\n",
    "# Standardize the feature columns\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(feature_columns)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=5)  # Select top 5 components\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Get names of the original variables\n",
    "original_variable_names = feature_columns.columns\n",
    "\n",
    "# Get the name of the top contributing variable for each component\n",
    "top_feature_names = []\n",
    "for i in range(5):\n",
    "    component = pca.components_[i]\n",
    "    top_feature_index = np.argmax(np.abs(component))\n",
    "    top_feature_name = original_variable_names[top_feature_index]\n",
    "    top_feature_names.append(top_feature_name)\n",
    "\n",
    "# Create a DataFrame using the principal components\n",
    "principal_df = pd.DataFrame(data=principal_components, columns=top_feature_names)\n",
    "\n",
    "# Concatenate the datetime and principal component dataframes\n",
    "final_df = pd.concat([datetime_column.reset_index(drop=True), principal_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries for MongoDB ingestion\n",
    "data_list = final_df.to_dict('records')\n",
    "\n",
    "# Show the first element that will be inserted into the MongoDB database for verification\n",
    "#print(f\"First element that will be inserted: {data_list[0]}\")\n",
    "\n",
    "# Insert each row as a document into the MongoDB collection\n",
    "for row in data_list:\n",
    "    collection.insert_one(row)\n",
    "\n",
    "print(f\"Inserted {len(data_list)} documents into the collection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel(\"./data/train_dataframes.xlsx\")\n",
    "\n",
    "# Assume we want to keep the 'datetime' column but perform PCA on all other columns.\n",
    "feature_columns = df.drop(['datetime'], axis=1)\n",
    "\n",
    "# Standardize the feature columns\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(feature_columns)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=5)  # Select top 5 components\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Get explained variance for the top 5 components\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Get names of the original variables\n",
    "original_variable_names = feature_columns.columns\n",
    "\n",
    "# Get the name of the top contributing variable for each component\n",
    "top_feature_names = []\n",
    "for i in range(5):\n",
    "    component = pca.components_[i]\n",
    "    top_feature_index = np.argmax(np.abs(component))\n",
    "    top_feature_name = original_variable_names[top_feature_index]\n",
    "    top_feature_names.append(top_feature_name)\n",
    "\n",
    "# Plotting the bar graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(top_feature_names, explained_variance, color='blue')\n",
    "plt.xlabel('Top Contributing Variables')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.title('Explained Variance by Top 5 Components')\n",
    "for i, v in enumerate(explained_variance):\n",
    "    plt.text(i, v, f\"{v:.4f}\")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
