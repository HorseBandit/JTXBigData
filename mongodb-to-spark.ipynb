{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pymongo) (2.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dnspython in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (1.26.0)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (2.1.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jerrod\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jerrod\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jerrod\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=0.25->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jerrod\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark\n",
    "%pip install pymongo\n",
    "%pip install dnspython\n",
    "%pip install numpy\n",
    "%pip install seaborn\n",
    "import seaborn as sns\n",
    "import pyspark\n",
    "import pymongo\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pyspark.sql.functions import collect_list, weekofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: findspark in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jerrod\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jerrod\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark findspark\n",
    "%pip install pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Electricity Load Prediction\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Excel file into a pandas DataFrame\n",
    "pandas_df = pd.read_excel(\"./data/train_dataframes.xlsx\")\n",
    "# List of columns you want to keep\n",
    "columns_to_keep = [\"datetime\", \"DEMAND\", \"MA_X-4\", \"holiday\", \"dayOfWeek\", \"hourOfDay\", \"T2M_toc\"]\n",
    "\n",
    "# Drop all other columns except the ones specified in 'columns_to_keep'\n",
    "pandas_df = pandas_df[columns_to_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize MongoDB Connection\n",
    "client = MongoClient('mongodb+srv://JTXBigData:pJRAyKW9QnqE7B1G@jtxbigdatacluster.dzo50pn.mongodb.net/')\n",
    "db = client['JTXBigDataCluster']\n",
    "collection = db['training-flattened']\n",
    "\n",
    "# Convert the Pandas DataFrame to a list of dictionaries\n",
    "data_to_insert = pandas_df.to_dict('records')\n",
    "\n",
    "# Ingest the data into the MongoDB collection\n",
    "collection.insert_many(data_to_insert)\n",
    "\n",
    "print(f\"Inserted {len(data_to_insert)} documents into the collection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jerrod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:479: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-----------------+-------+---------+---------+-----------------+\n",
      "|           datetime|           DEMAND|           MA_X-4|holiday|dayOfWeek|hourOfDay|          T2M_toc|\n",
      "+-------------------+-----------------+-----------------+-------+---------+---------+-----------------+\n",
      "|2015-01-31 01:00:00|         954.2018|        938.00485|      0|        1|        1|25.30849609375002|\n",
      "|2015-01-31 02:00:00|913.8660000000001|       900.284075|      0|        1|        2|25.14144287109377|\n",
      "|2015-01-31 03:00:00|         903.3637|881.7043249999999|      0|        1|        3|25.00673828125002|\n",
      "|2015-01-31 04:00:00|         889.0806|876.4588250000002|      0|        1|        4|24.89971313476565|\n",
      "|2015-01-31 05:00:00|         910.1472|       879.190775|      0|        1|        5|24.82155761718752|\n",
      "|2015-01-31 06:00:00|         922.1737|       877.027925|      0|        1|        6|24.83019409179689|\n",
      "|2015-01-31 07:00:00|         939.9442|       920.381925|      0|        1|        7|25.79995117187502|\n",
      "|2015-01-31 08:00:00|        1077.8575|      1057.194625|      0|        1|        8|26.98031005859377|\n",
      "|2015-01-31 09:00:00|        1179.6601|       1138.17875|      0|        1|        9|28.03182373046877|\n",
      "|2015-01-31 10:00:00|        1255.1569|      1189.291375|      0|        1|       10|28.90606079101565|\n",
      "|2015-01-31 11:00:00|        1253.4414|      1202.799725|      0|        1|       11| 29.5551696777344|\n",
      "|2015-01-31 12:00:00|        1223.6116|      1176.489125|      0|        1|       12|30.03652343750002|\n",
      "|2015-01-31 13:00:00|        1160.2838|      1153.509625|      0|        1|       13|30.26552734375002|\n",
      "|2015-01-31 14:00:00|        1124.8878|      1138.098025|      0|        1|       14|30.21114501953127|\n",
      "|2015-01-31 15:00:00|        1112.4189|       1124.01035|      0|        1|       15|29.73275146484377|\n",
      "|2015-01-31 16:00:00|        1081.7406|      1061.039175|      0|        1|       16| 28.9882751464844|\n",
      "|2015-01-31 17:00:00|        1064.8583|       929.245075|      0|        1|       17|27.99349365234377|\n",
      "|2015-01-31 18:00:00|        1095.5704|      1057.654125|      0|        1|       18|27.02187500000002|\n",
      "|2015-01-31 19:00:00|        1116.6654|       1107.91455|      0|        1|       19| 26.5200134277344|\n",
      "|2015-01-31 20:00:00|         1094.677|       1072.61775|      0|        1|       20|26.24984130859377|\n",
      "+-------------------+-----------------+-----------------+-------+---------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize MongoDB Connection\n",
    "client = MongoClient('mongodb+srv://JTXBigData:pJRAyKW9QnqE7B1G@jtxbigdatacluster.dzo50pn.mongodb.net/')\n",
    "db = client['JTXBigDataCluster']\n",
    "collection = db['training-flattened']\n",
    "\n",
    "# Retrieve data from MongoDB\n",
    "mongo_data = list(collection.find())\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "pandas_df = pd.DataFrame(mongo_data)\n",
    "\n",
    "# Drop the _id column provided by MongoDB\n",
    "if '_id' in pandas_df.columns:\n",
    "    pandas_df.drop('_id', axis=1, inplace=True)\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"MongoDBToSparkDF\").getOrCreate()\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "# Show the Spark DataFrame\n",
    "spark_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# feature columns are:\n",
    "# week_X-2 lagged features capable of capturing temporal trends\n",
    "# week_X-3\n",
    "# week_X-4\n",
    "\n",
    "# MA_X-4 moving average capable of capturing seasonal components\n",
    "# dayOfWeek daily and seasonal behavior\n",
    "# weekend\n",
    "# holiday\n",
    "# Holiday_ID\n",
    "# hourOfDay\n",
    "\n",
    "# T2M_toc weather feature\n",
    "\n",
    "# Define a list called 'feature_columns' containing the names of all the columns in the DataFrame\n",
    "# that we want to use as features for training our ML model.\n",
    "# These columns include lagged electricity demand data, moving average, and various time and weather variables.\n",
    "feature_columns = [\"MA_X-4\", \"dayOfWeek\", \"holiday\", \"hourOfDay\", \"T2M_toc\"]\n",
    "\n",
    "# Assemble features\n",
    "# Initialize the VectorAssembler transformer from Spark's MLlib library.\n",
    "# 'inputCols' specifies the list of column names that should be used as features.\n",
    "# 'outputCol' specifies the name of the new column that will contain the assembled feature vectors.\n",
    "# Output column is named 'assembled_features'.\n",
    "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"assembled_features\")\n",
    "\n",
    "# Scale features\n",
    "# Initialize the StandardScaler transformer from Spark's MLlib library.\n",
    "# 'inputCol' specifies the name of the column containing the assembled feature vectors that we want to scale.\n",
    "# In this case, the input column is 'assembled_features'.\n",
    "# 'outputCol' specifies the name of the new column that will contain the scaled feature vectors.\n",
    "# The new column is named 'features'.\n",
    "scaler = StandardScaler(inputCol=\"assembled_features\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Initialize a RandomForestRegressor model using Spark's MLlib.\n",
    "# This creates an instance of a Random Forest model tailored for regression tasks, aiming to predict a continuous variable.\n",
    "\n",
    "# 'featuresCol=\"features\"': \n",
    "# Tells the model to use the column named 'features' as the input feature vectors for training.\n",
    "# These feature vectors are numerical representations that encapsulate all the feature columns we've specified earlier.\n",
    "# They have been assembled and scaled, making them suitable for ML algorithms.\n",
    "# In Random Forest, these vectors are used by each decision tree in the forest to make individual predictions, which are then averaged.\n",
    "\n",
    "# 'labelCol=\"DEMAND\"': \n",
    "# Specifies that the column named 'DEMAND' contains the actual values of the target variable we are trying to predict.\n",
    "# This is what the model compares its predictions to during training in order to learn the best possible mapping from features to the target variable.\n",
    "# In the context of your problem, DEMAND likely represents the electricity load at each given hour, and the model will attempt to predict this load based on the input features.\n",
    "\n",
    "# In summary, we're preparing to train a Random Forest model to predict 'DEMAND' based on the assembled and scaled features.\n",
    "# Each tree in this Random Forest will look at these features to make its own prediction, and the final model output is typically an average of these individual tree predictions.\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"DEMAND\")\n",
    "\n",
    "# Create a Spark ML Pipeline for Random Forest Regression\n",
    "# The pipeline consists of three main stages:\n",
    "# 1. vector_assembler: Combines the specified feature columns into a single feature vector.\n",
    "# 2. scaler: Standardizes the feature vector to have zero mean and unit variance.\n",
    "# 3. rf: The RandomForestRegressor model that takes the scaled feature vector and performs regression to predict'DEMAND'.\n",
    "pipeline_rf = Pipeline(stages=[vector_assembler, scaler, rf])\n",
    "\n",
    "# MODEL ARTIFACT: Train the Random Forest model\n",
    "# Fit the Random Forest model using the pipeline\n",
    "# This performs the following actions in sequence:\n",
    "# 1. Assembles the feature columns into a single feature vector using 'vector_assembler'.\n",
    "# 2. Scales the assembled feature vector using 'scaler' to make each feature have zero mean and unit variance.\n",
    "# 3. Trains the Random Forest Regressor ('rf') using the scaled feature vectors and the label column ('DEMAND').\n",
    "# The resulting 'model_rf' object is a trained pipeline model, encapsulating learned knowledge.\n",
    "model_rf = pipeline_rf.fit(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jerrod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:479: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-----------------+-------+---------+---------+-----------------+\n",
      "|           datetime|   DEMAND|           MA_X-4|holiday|dayOfWeek|hourOfDay|          T2M_toc|\n",
      "+-------------------+---------+-----------------+-------+---------+---------+-----------------+\n",
      "|2019-04-13 01:00:00|1161.6177|       1118.15425|      0|        1|        1|26.79152832031252|\n",
      "|2019-04-13 02:00:00|1130.4635|       1078.06195|      0|        1|        2|26.70797119140627|\n",
      "|2019-04-13 03:00:00|1093.8777|      1040.469075|      0|        1|        3|26.61654052734377|\n",
      "|2019-04-13 04:00:00|1083.0332|      1021.725725|      0|        1|        4|26.54229125976565|\n",
      "|2019-04-13 05:00:00|1081.2705|         1021.294|      0|        1|        5|26.46718750000002|\n",
      "|2019-04-13 06:00:00|1034.6587|991.5679749999999|      0|        1|        6|26.55721435546877|\n",
      "|2019-04-13 07:00:00|1087.6133|      1058.302375|      0|        1|        7|27.20180664062502|\n",
      "|2019-04-13 08:00:00|1222.4729|      1186.062275|      0|        1|        8|28.30974731445315|\n",
      "|2019-04-13 09:00:00|1318.1572|      1279.055525|      0|        1|        9| 29.4951110839844|\n",
      "|2019-04-13 10:00:00|1390.8348|        1367.4495|      0|        1|       10|30.60363159179689|\n",
      "|2019-04-13 11:00:00|1422.3509|        1386.0266|      0|        1|       11|31.52697143554689|\n",
      "|2019-04-13 12:00:00|1403.7269|        1366.5949|      0|        1|       12|32.16378173828127|\n",
      "|2019-04-13 13:00:00|1381.3652|        1347.7561|      0|        1|       13|32.45351562500002|\n",
      "|2019-04-13 14:00:00|1378.3045|      1338.173425|      0|        1|       14|32.40798339843752|\n",
      "|2019-04-13 15:00:00|1352.1167|        1323.0593|      0|        1|       15|32.03554687500002|\n",
      "|2019-04-13 16:00:00|1323.7436|        1291.7624|      0|        1|       16|31.44121704101565|\n",
      "|2019-04-13 17:00:00|1273.1797|       1240.55525|      0|        1|       17| 30.5112243652344|\n",
      "|2019-04-13 18:00:00|1316.2446|       1254.14715|      0|        1|       18| 29.3378845214844|\n",
      "|2019-04-13 19:00:00|1364.0076|      1316.641375|      0|        1|       19|28.35512695312502|\n",
      "|2019-04-13 20:00:00| 1343.955|       1290.89385|      0|        1|       20|27.72731933593752|\n",
      "+-------------------+---------+-----------------+-------+---------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading Excel file into a pandas DataFrame\n",
    "pandas_df = pd.read_excel(\"./data/test_dataframes.xlsx\")\n",
    "# List of columns you want to keep\n",
    "columns_to_keep = [\"datetime\", \"DEMAND\", \"MA_X-4\", \"holiday\", \"dayOfWeek\", \"hourOfDay\", \"T2M_toc\"]\n",
    "\n",
    "# Drop all other columns except the ones specified in 'columns_to_keep'\n",
    "pandas_df = pandas_df[columns_to_keep]\n",
    "\n",
    "# Convert the Pandas DataFrame containing the test data to a Spark DataFrame\n",
    "# This is done using the 'createDataFrame' method of the SparkSession object ('spark').\n",
    "# The resulting Spark DataFrame ('spark_test_df') will be used for model evaluation.\n",
    "spark_test_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "spark_test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|   DEMAND|        prediction|\n",
      "+---------+------------------+\n",
      "|1161.6177|1075.7056039090621|\n",
      "|1130.4635|1057.5630453912077|\n",
      "|1093.8777|1030.4127739584899|\n",
      "|1083.0332|1006.8489051197878|\n",
      "|1081.2705|1007.7790248733224|\n",
      "|1034.6587| 977.4314868079249|\n",
      "|1087.6133|1081.2687110055324|\n",
      "|1222.4729|1212.5023653782525|\n",
      "|1318.1572|1273.7690960645434|\n",
      "|1390.8348|  1355.33181365864|\n",
      "|1422.3509|  1355.33181365864|\n",
      "|1403.7269|  1355.33181365864|\n",
      "|1381.3652|1348.8856218137312|\n",
      "|1378.3045|1348.8856218137312|\n",
      "|1352.1167|1301.8517641138228|\n",
      "|1323.7436|1296.2597251028803|\n",
      "|1273.1797|1243.5595294454818|\n",
      "|1316.2446|1270.5818891122321|\n",
      "|1364.0076|1276.3013435442313|\n",
      "| 1343.955|1271.9836683787328|\n",
      "+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform the test data using the pipeline\n",
    "# Use the trained Random Forest model to make predictions on the test data\n",
    "# The 'transform' method applies all the stages of the pipeline to the test data:\n",
    "# 1. 'vector_assembler' assembles the feature columns into a single feature vector.\n",
    "# 2. 'scaler' scales the assembled feature vector.\n",
    "# 3. 'rf' (Random Forest Regressor) makes the predictions based on the scaled feature vector.\n",
    "# The resulting DataFrame ('transformed_test') contains a new column called 'prediction' with the predicted values for 'DEMAND'.\n",
    "# Prediction Phase: We then pass this new, unseen test data through the trained model by calling\n",
    "# the transform method on our pipeline model. What happens here is similar to the training phase in that the features are assembled and scaled. \n",
    "# Based on the learned patterns/rules, the model makes new predictions for each hour of this test data.\n",
    "transformed_test = model_rf.transform(spark_test_df)\n",
    "\n",
    "# Show the predictions\n",
    "transformed_test.select(\"DEMAND\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 91.83037216074906\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize the evaluator\n",
    "# Create a RegressionEvaluator object for model evaluation\n",
    "# This object is configured to use the 'DEMAND' column as the label (true values) and the 'prediction' column for predicted values.\n",
    "# The RegressionEvaluator will calculate metrics like RMSE, MAE, etc., to quantify the performance of the model on the test data.\n",
    "evaluator = RegressionEvaluator(labelCol=\"DEMAND\", predictionCol=\"prediction\")\n",
    "\n",
    "# Compute the RMSE on the test data\n",
    "# Calculate the Root Mean Squared Error (RMSE) of the model on the test data\n",
    "# This is done using the 'evaluate' method of the 'evaluator' object.\n",
    "# The 'metricName' parameter is set to \"rmse\" to specify that we want to calculate the RMSE.\n",
    "# The result is stored in the variable 'rmse'.\n",
    "rmse = evaluator.evaluate(transformed_test, {evaluator.metricName: \"rmse\"})\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
