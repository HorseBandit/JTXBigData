{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pymongo in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pymongo) (2.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: dnspython in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting numpy\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/cc/05/ef9fc04adda45d537619ea956bc33489f50a46badc949c4280d8309185ec/numpy-1.26.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading numpy-1.26.0-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 61.1/61.1 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.0-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 15.8/15.8 MB 16.8 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.26.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark\n",
    "%pip install pymongo\n",
    "%pip install dnspython\n",
    "%pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pymongo\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mongo-spark-connector in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (10.0.3)\n",
      "Requirement already satisfied: peppercorn in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mongo-spark-connector) (0.6)\n",
      "Requirement already satisfied: requests in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mongo-spark-connector) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->mongo-spark-connector) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->mongo-spark-connector) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->mongo-spark-connector) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jerrod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->mongo-spark-connector) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mongo-spark-connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering:\n",
      "+-------------------+----------+------------------+-----------+------------+------------------+------------------+-----------+------------+------------------+------------------+-----------+-----------+------------------+----------+-------+------+\n",
      "|datetime           |nat_demand|T2M_toc           |QV2M_toc   |TQL_toc     |W2M_toc           |T2M_san           |QV2M_san   |TQL_san     |W2M_san           |T2M_dav           |QV2M_dav   |TQL_dav    |W2M_dav           |Holiday_ID|holiday|school|\n",
      "+-------------------+----------+------------------+-----------+------------+------------------+------------------+-----------+------------+------------------+------------------+-----------+-----------+------------------+----------+-------+------+\n",
      "|2015-01-03 01:00:00|970.345   |25.865258789062523|0.018576382|0.016174316 |21.85054581787518 |23.482446289062523|0.017271755|0.0018553734|10.328948729384228|22.662133789062523|0.016562222|0.09609985 |5.364147952093894 |0         |0      |0     |\n",
      "|2015-01-03 02:00:00|912.1755  |25.899255371093773|0.018653292|0.016418457 |22.166944276988236|23.399255371093773|0.017264742|0.0013270378|10.681517137074179|22.578942871093773|0.016509432|0.087646484|5.5724713462616595|0         |0      |0     |\n",
      "|2015-01-03 03:00:00|900.2688  |25.937280273437523|0.01876786 |0.0154800415|22.454910876263515|23.343530273437523|0.017211463|0.0014281273|10.874923789974078|22.531030273437523|0.016479041|0.07873535 |5.871183745099925 |0         |0      |0     |\n",
      "|2015-01-03 04:00:00|889.9538  |25.957543945312523|0.018890057|0.016273499 |22.110480868986375|23.238793945312523|0.017127667|0.0025987625|10.518620160775363|22.512231445312523|0.016486797|0.06838989 |5.883621114044925 |0         |0      |0     |\n",
      "|2015-01-03 05:00:00|893.6865  |25.973840332031273|0.018981151|0.017280579 |21.186088517696877|23.075402832031273|0.017058544|0.0017290115|9.73358891774275  |22.481652832031273|0.016455822|0.06436157 |5.6117235593949095|0         |0      |0     |\n",
      "|2015-01-03 06:00:00|879.2323  |26.034143066406273|0.019079996|0.014541626 |20.06203781593797 |22.995080566406273|0.01702769 |0.0014853477|9.087273159632828 |22.456018066406273|0.016409708|0.061538696|5.280351132074735 |0         |0      |0     |\n",
      "|2015-01-03 07:00:00|932.4876  |26.691491699218773|0.019331587|0.0066452026|21.62349567450987 |24.285241699218773|0.017424239|0.0021762848|11.395393424647095|22.949304199218773|0.016569747|0.060897827|5.126910803396752 |0         |0      |0     |\n",
      "|2015-01-03 08:00:00|1048.972  |27.674066162109398|0.019369703|0.0068626404|23.775316995619374|26.189691162109398|0.018072706|0.0045394897|12.872865998530138|24.088128662109398|0.016676527|0.05619812 |5.060610609341514 |0         |0      |0     |\n",
      "|2015-01-03 09:00:00|1167.9074 |28.760400390625023|0.019171301|0.010231018 |24.636151504191965|27.916650390625023|0.018454138|0.0042915344|14.548026668797892|25.479150390625023|0.016645972|0.051071167|4.915658462517222 |0         |0      |0     |\n",
      "|2015-01-03 10:00:00|1257.5069 |29.766656494140648|0.018759238|0.009017944 |25.86267135806722 |29.172906494140648|0.018675314|0.0049209595|15.081687878521313|26.704156494140648|0.016607748|0.058685303|4.685579709203943 |0         |0      |0     |\n",
      "+-------------------+----------+------------------+-----------+------------+------------------+------------------+-----------+------------+------------------+------------------+-----------+-----------+------------------+----------+-------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "After filtering:\n",
      "+-------------------+----------+\n",
      "|datetime           |nat_demand|\n",
      "+-------------------+----------+\n",
      "|2015-01-03 01:00:00|970.345   |\n",
      "|2015-01-03 02:00:00|912.1755  |\n",
      "|2015-01-03 03:00:00|900.2688  |\n",
      "|2015-01-03 04:00:00|889.9538  |\n",
      "|2015-01-03 05:00:00|893.6865  |\n",
      "|2015-01-03 06:00:00|879.2323  |\n",
      "|2015-01-03 07:00:00|932.4876  |\n",
      "|2015-01-03 08:00:00|1048.972  |\n",
      "|2015-01-03 09:00:00|1167.9074 |\n",
      "|2015-01-03 10:00:00|1257.5069 |\n",
      "+-------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "file_path = \"./data/continuous dataset.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the first 10 rows before filtering\n",
    "print(\"Before filtering:\")\n",
    "df.show(10, truncate=False)\n",
    "\n",
    "# Select only the datetime and nat_demand columns\n",
    "df_filtered = df.select(\"datetime\", \"nat_demand\")\n",
    "\n",
    "# Show the first 10 rows after filtering\n",
    "print(\"After filtering:\")\n",
    "df_filtered.show(10, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+----+----------+----------+\n",
      "|           datetime|day_of_week|hour|nat_demand|prediction|\n",
      "+-------------------+-----------+----+----------+----------+\n",
      "|2015-01-03 01:00:00|          7|   1|   970.345|         0|\n",
      "|2015-01-03 02:00:00|          7|   2|  912.1755|         0|\n",
      "|2015-01-03 03:00:00|          7|   3|  900.2688|         0|\n",
      "|2015-01-03 04:00:00|          7|   4|  889.9538|         0|\n",
      "|2015-01-03 05:00:00|          7|   5|  893.6865|         0|\n",
      "|2015-01-03 06:00:00|          7|   6|  879.2323|         0|\n",
      "|2015-01-03 07:00:00|          7|   7|  932.4876|         0|\n",
      "|2015-01-03 08:00:00|          7|   8|  1048.972|         2|\n",
      "|2015-01-03 09:00:00|          7|   9| 1167.9074|         4|\n",
      "|2015-01-03 10:00:00|          7|  10| 1257.5069|         4|\n",
      "|2015-01-03 11:00:00|          7|  11|  1254.583|         4|\n",
      "|2015-01-03 12:00:00|          7|  12| 1216.9004|         4|\n",
      "|2015-01-03 13:00:00|          7|  13| 1202.1556|         4|\n",
      "|2015-01-03 14:00:00|          7|  14| 1197.2616|         4|\n",
      "|2015-01-03 15:00:00|          7|  15| 1169.0034|         4|\n",
      "|2015-01-03 16:00:00|          7|  16| 1136.7054|         2|\n",
      "|2015-01-03 17:00:00|          7|  17| 1101.9447|         2|\n",
      "|2015-01-03 18:00:00|          7|  18| 1107.0406|         2|\n",
      "|2015-01-03 19:00:00|          7|  19| 1142.1548|         2|\n",
      "|2015-01-03 20:00:00|          7|  20| 1097.2334|         2|\n",
      "+-------------------+-----------+----+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Cluster Centers: \n",
      "[  3.70736946   4.7948612  949.36992476]\n",
      "[   4.33553998   14.22988058 1363.83867426]\n",
      "[   3.78567686   11.43842795 1089.89207482]\n",
      "[   3.9892403    12.6765569  1505.69205728]\n",
      "[   4.32629505   16.53209459 1231.33508919]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import dayofweek\n",
    "\n",
    "# Add time-based features\n",
    "df_filtered = df_filtered.withColumn(\"day_of_week\", dayofweek(\"datetime\"))\n",
    "df_filtered = df_filtered.withColumn(\"hour\", hour(\"datetime\"))\n",
    "\n",
    "# Assemble features into a single vector\n",
    "feature_columns = [\"day_of_week\", \"hour\", \"nat_demand\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df_features = assembler.transform(df_filtered)\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans = KMeans().setK(5).setSeed(1)\n",
    "model = kmeans.fit(df_features)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(df_features)\n",
    "\n",
    "# Show the resulting clusters\n",
    "predictions.select(\"datetime\", \"day_of_week\", \"hour\", \"nat_demand\", \"prediction\").show(20)\n",
    "\n",
    "# You can also examine the cluster centers\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+----------+\n",
      "|prediction|        avg_demand|        std_demand|        min_demand|max_demand|\n",
      "+----------+------------------+------------------+------------------+----------+\n",
      "|         0| 948.6941679945553| 51.70354086833256| 85.19250000000002| 1019.9557|\n",
      "|         2|1088.1731736523966|40.664910880681205|          1019.453| 1160.9232|\n",
      "|         4| 1229.164280808098| 39.36214528566216|         1160.3637| 1297.6514|\n",
      "|         1|1361.6491870616849| 39.92515129585863|1297.6509999999998| 1434.7894|\n",
      "|         3|1504.7913107896306| 52.11418229897436|1434.7267000000002|  1754.882|\n",
      "+----------+------------------+------------------+------------------+----------+\n",
      "\n",
      "Peak Demand Times:\n",
      "+-------------------+----------+\n",
      "|           datetime|nat_demand|\n",
      "+-------------------+----------+\n",
      "|2015-01-06 13:00:00|   1437.47|\n",
      "|2015-01-12 11:00:00|  1452.004|\n",
      "|2015-01-12 12:00:00| 1439.2499|\n",
      "|2015-01-12 13:00:00| 1468.9201|\n",
      "|2015-01-12 14:00:00| 1466.2352|\n",
      "|2015-01-14 13:00:00| 1452.8792|\n",
      "|2015-01-14 14:00:00| 1453.3303|\n",
      "|2015-01-15 14:00:00| 1439.2714|\n",
      "|2015-01-16 13:00:00| 1439.7809|\n",
      "|2015-01-16 14:00:00| 1442.2483|\n",
      "|2015-02-02 11:00:00| 1462.1711|\n",
      "|2015-02-02 12:00:00| 1459.8766|\n",
      "|2015-02-02 13:00:00| 1485.6554|\n",
      "|2015-02-02 14:00:00|  1495.018|\n",
      "|2015-02-02 15:00:00| 1457.9692|\n",
      "|2015-02-03 11:00:00| 1447.1545|\n",
      "|2015-02-03 12:00:00| 1443.6939|\n",
      "|2015-02-03 13:00:00| 1471.9474|\n",
      "|2015-02-03 14:00:00| 1476.5739|\n",
      "|2015-02-03 15:00:00| 1456.7079|\n",
      "+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Low Demand Times:\n",
      "+-------------------+----------+\n",
      "|           datetime|nat_demand|\n",
      "+-------------------+----------+\n",
      "|2015-01-03 01:00:00|   970.345|\n",
      "|2015-01-03 02:00:00|  912.1755|\n",
      "|2015-01-03 03:00:00|  900.2688|\n",
      "|2015-01-03 04:00:00|  889.9538|\n",
      "|2015-01-03 05:00:00|  893.6865|\n",
      "|2015-01-03 06:00:00|  879.2323|\n",
      "|2015-01-03 07:00:00|  932.4876|\n",
      "|2015-01-03 23:00:00|   999.634|\n",
      "|2015-01-04 00:00:00|  968.0526|\n",
      "|2015-01-04 01:00:00|  944.0556|\n",
      "|2015-01-04 02:00:00|  928.7193|\n",
      "|2015-01-04 03:00:00|  909.5566|\n",
      "|2015-01-04 04:00:00|  894.6543|\n",
      "|2015-01-04 05:00:00|  884.6659|\n",
      "|2015-01-04 06:00:00|  862.9888|\n",
      "|2015-01-04 07:00:00|  871.6873|\n",
      "|2015-01-04 08:00:00|  912.4966|\n",
      "|2015-01-04 09:00:00|  969.8017|\n",
      "|2015-01-04 10:00:00| 1019.3105|\n",
      "|2015-01-04 23:00:00|  1012.596|\n",
      "+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "# Group by cluster and calculate average and standard deviation\n",
    "cluster_stats = predictions.groupBy(\"prediction\").agg(\n",
    "    F.mean(\"nat_demand\").alias(\"avg_demand\"),\n",
    "    F.stddev(\"nat_demand\").alias(\"std_demand\"),\n",
    "    F.min(\"nat_demand\").alias(\"min_demand\"),\n",
    "    F.max(\"nat_demand\").alias(\"max_demand\")\n",
    ").orderBy(\"avg_demand\")\n",
    "\n",
    "# Show cluster statistics\n",
    "cluster_stats.show()\n",
    "\n",
    "# Identify peak and low demand clusters\n",
    "peak_cluster = cluster_stats.orderBy(F.desc(\"avg_demand\")).first().prediction\n",
    "low_cluster = cluster_stats.orderBy(\"avg_demand\").first().prediction\n",
    "\n",
    "# Extract times falling into peak and low demand clusters\n",
    "peak_times = predictions.filter(predictions.prediction == peak_cluster).select(\"datetime\", \"nat_demand\")\n",
    "low_times = predictions.filter(predictions.prediction == low_cluster).select(\"datetime\", \"nat_demand\")\n",
    "\n",
    "# Show peak and low demand times\n",
    "print(\"Peak Demand Times:\")\n",
    "peak_times.show()\n",
    "\n",
    "print(\"Low Demand Times:\")\n",
    "low_times.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z-Scores:\n",
      "+-------------------+----------+----------+--------------------+\n",
      "|           datetime|nat_demand|prediction|             z_score|\n",
      "+-------------------+----------+----------+--------------------+\n",
      "|2015-01-03 01:00:00|   970.345|         0|  0.4187495022938207|\n",
      "|2015-01-03 02:00:00|  912.1755|         0| -0.7063088403859116|\n",
      "|2015-01-03 03:00:00|  900.2688|         0| -0.9365967433038458|\n",
      "|2015-01-03 04:00:00|  889.9538|         0| -1.1360995206139741|\n",
      "|2015-01-03 05:00:00|  893.6865|         0| -1.0639052388044232|\n",
      "|2015-01-03 06:00:00|  879.2323|         0| -1.3434644287022461|\n",
      "|2015-01-03 07:00:00|  932.4876|         0|-0.31345180083176405|\n",
      "|2015-01-03 23:00:00|   999.634|         0|   0.985229080057884|\n",
      "|2015-01-04 00:00:00|  968.0526|         0|  0.3744121133742139|\n",
      "|2015-01-04 01:00:00|  944.0556|         0|-0.08971470651052814|\n",
      "|2015-01-04 02:00:00|  928.7193|         0|  -0.386334623491751|\n",
      "|2015-01-04 03:00:00|  909.5566|         0| -0.7569610772736967|\n",
      "|2015-01-04 04:00:00|  894.6543|         0| -1.0451869850108466|\n",
      "|2015-01-04 05:00:00|  884.6659|         0| -1.2383729802492847|\n",
      "|2015-01-04 06:00:00|  862.9888|         0| -1.6576305327484953|\n",
      "|2015-01-04 07:00:00|  871.6873|         0| -1.4893925387172633|\n",
      "|2015-01-04 08:00:00|  912.4966|         0| -0.7000984340074085|\n",
      "|2015-01-04 09:00:00|  969.8017|         0| 0.40824151791063107|\n",
      "|2015-01-04 10:00:00| 1019.3105|         0|  1.3657929576868442|\n",
      "|2015-01-04 23:00:00|  1012.596|         0|   1.235927577342796|\n",
      "+-------------------+----------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Anomalous High Demand Times:\n",
      "+-------------------+------------------+------------------+\n",
      "|           datetime|        nat_demand|           z_score|\n",
      "+-------------------+------------------+------------------+\n",
      "|2019-03-21 13:00:00|         1719.0439|4.1112146398311635|\n",
      "|2019-03-21 14:00:00|1716.3082000000002| 4.058720292240611|\n",
      "|2019-04-29 13:00:00|1690.4549000000002| 3.562630766136521|\n",
      "|2019-05-02 11:00:00|         1689.2014|3.5385778127042085|\n",
      "|2019-05-02 13:00:00|         1706.3962| 3.868522546392156|\n",
      "|2019-05-02 14:00:00|         1713.8466| 4.011485549385429|\n",
      "|2019-05-02 15:00:00|         1689.5138| 3.544572342143581|\n",
      "|2019-05-06 13:00:00|         1690.0908|3.5556441842899242|\n",
      "|2019-05-07 13:00:00|         1687.5812|3.5074883869754325|\n",
      "|2019-05-07 14:00:00|         1702.5714|3.7951298568924456|\n",
      "|2019-06-18 12:00:00|         1694.8607|3.6471720523208493|\n",
      "|2019-06-18 13:00:00|1709.3298000000002|3.9248143247639344|\n",
      "|2019-06-18 14:00:00|         1703.2183| 3.807542984595165|\n",
      "|2019-08-27 14:00:00|1696.8455000000001|3.6852576542135083|\n",
      "|2019-11-27 14:00:00|         1688.5056|3.5252263607710623|\n",
      "|2019-12-10 13:00:00|         1704.8151|3.8381833962750287|\n",
      "|2019-12-17 13:00:00|         1695.6473|3.6622658322728685|\n",
      "|2019-12-17 14:00:00|1721.5157000000002| 4.158645106758953|\n",
      "|2020-01-17 13:00:00|         1734.0464| 4.399092129953378|\n",
      "|2020-01-17 14:00:00|          1754.882| 4.798898844380335|\n",
      "+-------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Anomalous Low Demand Times:\n",
      "+-------------------+----------+-------------------+\n",
      "|           datetime|nat_demand|            z_score|\n",
      "+-------------------+----------+-------------------+\n",
      "|2015-01-24 17:00:00|  509.8358| -8.487975110102187|\n",
      "|2015-02-15 02:00:00|  721.5908| -4.392414217295017|\n",
      "|2015-02-15 03:00:00|  726.2926| -4.301476538346227|\n",
      "|2015-02-15 04:00:00|  739.3178|-4.0495556876414405|\n",
      "|2015-03-22 06:00:00|  763.7878| -3.576280558142769|\n",
      "|2015-06-01 01:00:00|  766.4758|-3.5242918557278737|\n",
      "|2015-06-01 02:00:00|  705.4952| -4.703719782246338|\n",
      "|2015-06-01 03:00:00|  666.7528| -5.453037901457184|\n",
      "|2015-06-01 04:00:00|  682.5229| -5.148027843438914|\n",
      "|2015-06-01 05:00:00|  708.4881| -4.645833998221957|\n",
      "|2015-06-01 06:00:00|  741.1823| -4.013494327651645|\n",
      "|2015-06-19 02:00:00|   720.303|  -4.41732160232846|\n",
      "|2015-06-19 03:00:00|  674.4172|-5.3048004718482895|\n",
      "|2015-06-19 04:00:00|  701.7733| -4.775705180876543|\n",
      "|2015-06-19 05:00:00|  732.2891| -4.185498021221635|\n",
      "|2015-09-23 00:00:00|  766.2125|-3.5293843502761795|\n",
      "|2015-09-23 01:00:00|  751.0318|-3.8229948021920155|\n",
      "|2015-09-23 02:00:00|  734.9152| -4.134706528880953|\n",
      "|2015-09-23 03:00:00|  708.8303| -4.639215496002324|\n",
      "|2015-09-23 04:00:00|  720.1334| -4.420601841885589|\n",
      "+-------------------+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Calculate Z-scores for demand in each cluster\n",
    "z_score_window = Window.partitionBy('prediction')\n",
    "predictions_with_z_score = predictions.withColumn(\n",
    "    'avg_demand',\n",
    "    F.avg('nat_demand').over(z_score_window)\n",
    ").withColumn(\n",
    "    'std_demand',\n",
    "    F.stddev('nat_demand').over(z_score_window)\n",
    ").withColumn(\n",
    "    'z_score',\n",
    "    (F.col('nat_demand') - F.col('avg_demand')) / F.col('std_demand')\n",
    ")\n",
    "\n",
    "# Show Z-scores\n",
    "print(\"Z-Scores:\")\n",
    "predictions_with_z_score.select('datetime', 'nat_demand', 'prediction', 'z_score').show()\n",
    "\n",
    "# Identify times when demand is unusually high or low\n",
    "anomalous_high = predictions_with_z_score.filter(F.col('z_score') > 3.5).select('datetime', 'nat_demand', 'z_score')\n",
    "anomalous_low = predictions_with_z_score.filter(F.col('z_score') < -3.5).select('datetime', 'nat_demand', 'z_score')\n",
    "\n",
    "# Show anomalous times\n",
    "print(\"Anomalous High Demand Times:\")\n",
    "anomalous_high.show()\n",
    "\n",
    "print(\"Anomalous Low Demand Times:\")\n",
    "anomalous_low.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         0|11990|\n",
      "|         1| 7798|\n",
      "|         2|11401|\n",
      "|         3| 6214|\n",
      "|         4|10645|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of each unique value in the 'prediction' column\n",
    "cluster_distribution = predictions.groupBy(\"prediction\").count().orderBy(\"prediction\")\n",
    "\n",
    "# Show the distribution\n",
    "cluster_distribution.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The date with the most anomalous demand times is: 2020-02-28\n",
      "The date with the highest load is: 2020-01-20\n",
      "The date with the lowest load is: 2015-06-27\n"
     ]
    }
   ],
   "source": [
    "# Extract the month, year, and date from the datetime column for each DataFrame\n",
    "from pyspark.sql.functions import to_date\n",
    "anomalous_high = anomalous_high.withColumn('date', to_date('datetime'))\n",
    "peak_times = peak_times.withColumn('date', to_date('datetime'))\n",
    "low_times = low_times.withColumn('date', to_date('datetime'))\n",
    "\n",
    "# Find the date with the most anomalous demand times\n",
    "most_anomalous_date = anomalous_high.groupBy('date').count().orderBy(F.desc('count')).first().date\n",
    "\n",
    "# Find the date with the highest load\n",
    "highest_load_date = peak_times.groupBy('date').count().orderBy(F.desc('count')).first().date\n",
    "\n",
    "# Find the date with the lowest load\n",
    "lowest_load_date = low_times.groupBy('date').count().orderBy('count').first().date\n",
    "\n",
    "# Display the results\n",
    "print(f\"The date with the most anomalous demand times is: {most_anomalous_date}\")\n",
    "print(f\"The date with the highest load is: {highest_load_date}\")\n",
    "print(f\"The date with the lowest load is: {lowest_load_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 3 dates with the most anomalous demand times are: \n",
      "  - 2020-02-28\n",
      "  - 2019-05-02\n",
      "  - 2020-01-17\n",
      "The top 3 dates with the highest load are: \n",
      "  - 2020-01-20\n",
      "  - 2019-05-07\n",
      "  - 2020-01-21\n",
      "The top 3 dates with the lowest load are: \n",
      "  - 2016-05-17\n",
      "  - 2018-03-17\n",
      "  - 2016-04-23\n"
     ]
    }
   ],
   "source": [
    "# Find the top 3 dates with the most anomalous demand times\n",
    "top_anomalous_dates = anomalous_high.groupBy('date').count().orderBy(F.desc('count')).limit(3).collect()\n",
    "\n",
    "# Find the top 3 dates with the highest load\n",
    "top_highest_load_dates = peak_times.groupBy('date').count().orderBy(F.desc('count')).limit(3).collect()\n",
    "\n",
    "# Find the top 3 dates with the lowest load\n",
    "top_lowest_load_dates = low_times.groupBy('date').count().orderBy('count').limit(3).collect()\n",
    "\n",
    "# Display the results\n",
    "print(\"The top 3 dates with the most anomalous demand times are: \")\n",
    "for row in top_anomalous_dates:\n",
    "    print(f\"  - {row.date}\")\n",
    "\n",
    "print(\"The top 3 dates with the highest load are: \")\n",
    "for row in top_highest_load_dates:\n",
    "    print(f\"  - {row.date}\")\n",
    "\n",
    "print(\"The top 3 dates with the lowest load are: \")\n",
    "for row in top_lowest_load_dates:\n",
    "    print(f\"  - {row.date}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
