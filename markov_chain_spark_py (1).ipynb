{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.metrics import r2_score\n",
    "from pyspark.sql.functions import month, year\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import distinctipy\n",
    "from math import ceil\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "\n",
    "# Create 'out/' directory if it doesn't exist\n",
    "if not os.path.exists('out'):\n",
    "    os.makedirs('out')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to and retrieving data from the database.\n",
      "Finished in 4.087 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjyua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\sql\\pandas\\conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data as numpy array\n",
      "Type = <class 'numpy.ndarray'>\n",
      "[[9.1014720e+02 1.0000000e+00 2.0150000e+03]\n",
      " [1.1602838e+03 1.0000000e+00 2.0150000e+03]\n",
      " [1.1796601e+03 1.0000000e+00 2.0150000e+03]\n",
      " [9.0336370e+02 1.0000000e+00 2.0150000e+03]\n",
      " [1.2498193e+03 2.0000000e+00 2.0150000e+03]]\n",
      "\n",
      "m = 1: 44542 samples (28898 \"outliers\" removed)\n",
      "m = 2: 72608 samples (832 \"outliers\" removed)\n",
      "m = 3: 73426 samples (14 \"outliers\" removed)\n",
      "m = 4: 73432 samples (8 \"outliers\" removed)\n",
      "m = 5: 73436 samples (4 \"outliers\" removed)\n",
      "m = 6: 73440 samples (0 \"outliers\" removed)\n",
      "\n",
      "Original sample count: 73440\n",
      "Downsized to chosen  : 72608 (832 removed)\n",
      "Load changes: shape = (72607,)\n",
      "[ 250.1366   19.3763 -276.2964 ...   72.811    -6.6065   22.3359]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjyua\\AppData\\Local\\Temp\\ipykernel_103536\\3215733184.py:336: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.savefig(save_path + \".png\", format='png')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negatives: Min = -688.39960, Q1 = -267.15210, Q2 = -156.38460, Q3 = -66.80100, Max = -0.02230\n",
      "No changes: count = 0\n",
      "Positives: Min = 0.01300, Q1 = 71.90405, Q2 = 180.21475, Q3 = 320.37937, Max = 690.09340\n",
      "All targets:\n",
      "[-267.152, -156.385, -66.801, 0, 71.904, 180.215, 320.379]\n",
      "Starting training\n",
      "Finished in 469.495 seconds.r2 = -2.7448217360938763\n",
      "Years: [2015. 2016. 2017. 2018. 2019.]\n",
      "Counts: [16078 17568 17520 17520  4754]\n",
      "Markov Chain instance saved to 'out/post_train_markov_chain_model.pkl'.\n",
      "Sample index: <predicted load> vs. <actual load> (<percentage diff> %)\n",
      "0: 1340.50 vs. 1179.66 (13.63 % diff)\n",
      "1000: 1558.39 vs. 1195.62 (30.34 % diff)\n",
      "2000: 900.52 vs. 1103.28 (18.38 % diff)\n",
      "3000: 699.20 vs. 1147.10 (39.05 % diff)\n",
      "4000: 1119.33 vs. 1129.32 (0.88 % diff)\n",
      "5000: 987.03 vs. 1185.04 (16.71 % diff)\n",
      "6000: 1067.69 vs. 1443.48 (26.03 % diff)\n",
      "7000: 624.76 vs. 923.85 (32.37 % diff)\n",
      "8000: 732.53 vs. 1311.23 (44.13 % diff)\n",
      "9000: 658.49 vs. 864.26 (23.81 % diff)\n",
      "10000: 1619.17 vs. 1181.88 (37.00 % diff)\n",
      "11000: 720.64 vs. 1505.50 (52.13 % diff)\n",
      "12000: 1001.14 vs. 1044.40 (4.14 % diff)\n",
      "13000: 780.72 vs. 1551.98 (49.70 % diff)\n",
      "14000: 1746.41 vs. 1505.46 (16.01 % diff)\n",
      "15000: 642.55 vs. 892.02 (27.97 % diff)\n",
      "16000: 965.92 vs. 1368.45 (29.41 % diff)\n",
      "17000: 822.74 vs. 1110.43 (25.91 % diff)\n",
      "18000: 935.73 vs. 1047.84 (10.70 % diff)\n",
      "19000: 663.44 vs. 1445.84 (54.11 % diff)\n",
      "20000: 1772.99 vs. 1267.71 (39.86 % diff)\n",
      "21000: 1076.02 vs. 1119.43 (3.88 % diff)\n",
      "22000: 1388.77 vs. 962.53 (44.28 % diff)\n",
      "23000: 741.04 vs. 1051.10 (29.50 % diff)\n",
      "24000: 1216.52 vs. 1086.80 (11.94 % diff)\n",
      "25000: 1406.84 vs. 1036.03 (35.79 % diff)\n",
      "26000: 1473.19 vs. 999.21 (47.44 % diff)\n",
      "27000: 1346.16 vs. 1254.74 (7.29 % diff)\n",
      "28000: 802.44 vs. 1460.73 (45.07 % diff)\n",
      "29000: 1307.66 vs. 1192.03 (9.70 % diff)\n",
      "30000: 1075.40 vs. 1033.68 (4.04 % diff)\n",
      "31000: 656.01 vs. 1234.50 (46.86 % diff)\n",
      "32000: 1492.82 vs. 1127.28 (32.43 % diff)\n",
      "33000: 759.65 vs. 1436.00 (47.10 % diff)\n",
      "34000: 1870.89 vs. 1344.87 (39.11 % diff)\n",
      "35000: 1619.07 vs. 1181.28 (37.06 % diff)\n",
      "36000: 1067.00 vs. 1115.23 (4.32 % diff)\n",
      "37000: 563.26 vs. 867.84 (35.10 % diff)\n",
      "38000: 1780.12 vs. 1449.29 (22.83 % diff)\n",
      "39000: 1304.03 vs. 978.24 (33.30 % diff)\n",
      "40000: 1509.30 vs. 1221.11 (23.60 % diff)\n",
      "41000: 578.95 vs. 921.83 (37.20 % diff)\n",
      "42000: 1189.58 vs. 1065.96 (11.60 % diff)\n",
      "43000: 1249.63 vs. 1021.47 (22.34 % diff)\n",
      "44000: 1174.92 vs. 805.26 (45.91 % diff)\n",
      "45000: 1336.27 vs. 1143.60 (16.85 % diff)\n",
      "46000: 782.01 vs. 1136.06 (31.17 % diff)\n",
      "47000: 650.25 vs. 1017.75 (36.11 % diff)\n",
      "48000: 733.95 vs. 1204.49 (39.07 % diff)\n",
      "49000: 1148.16 vs. 1226.91 (6.42 % diff)\n",
      "50000: 1722.90 vs. 929.72 (85.31 % diff)\n",
      "51000: 1323.56 vs. 1247.22 (6.12 % diff)\n",
      "52000: 1743.47 vs. 1102.38 (58.16 % diff)\n",
      "53000: 1680.30 vs. 1241.31 (35.36 % diff)\n",
      "54000: 646.30 vs. 1207.96 (46.50 % diff)\n",
      "55000: 1318.44 vs. 1249.51 (5.52 % diff)\n",
      "56000: 1501.76 vs. 1318.99 (13.86 % diff)\n",
      "57000: 700.47 vs. 960.36 (27.06 % diff)\n",
      "58000: 800.69 vs. 1389.72 (42.38 % diff)\n",
      "59000: 1042.26 vs. 1153.72 (9.66 % diff)\n",
      "60000: 1015.28 vs. 1106.33 (8.23 % diff)\n",
      "61000: 1690.23 vs. 1219.46 (38.60 % diff)\n",
      "62000: 967.42 vs. 1258.16 (23.11 % diff)\n",
      "63000: 1048.02 vs. 1050.87 (0.27 % diff)\n",
      "64000: 1251.67 vs. 1227.21 (1.99 % diff)\n",
      "65000: 978.81 vs. 1230.38 (20.45 % diff)\n",
      "66000: 1334.92 vs. 1268.63 (5.22 % diff)\n",
      "67000: 1479.85 vs. 1237.06 (19.63 % diff)\n",
      "68000: 1478.94 vs. 1516.65 (2.49 % diff)\n",
      "69000: 1052.47 vs. 1236.12 (14.86 % diff)\n",
      "70000: 846.59 vs. 1022.18 (17.18 % diff)\n",
      "71000: 814.59 vs. 870.85 (6.46 % diff)\n",
      "72000: 1070.50 vs. 1184.19 (9.60 % diff)\n",
      "73000: 1579.49 vs. 1201.07 (31.51 % diff)\n",
      "MSE Markov Chain while predicting one sample ahead during training = 138087.5682669396.\n",
      "\n",
      "<class 'numpy.ndarray'>: shape = (168, 3)\n",
      "[[1161.6177    4.     2019.    ]\n",
      " [1130.4635    4.     2019.    ]\n",
      " [1093.8777    4.     2019.    ]\n",
      " [1083.0332    4.     2019.    ]\n",
      " [1081.2705    4.     2019.    ]]\n",
      "Ellipsis\n",
      "[[1169.1743    4.     2019.    ]\n",
      " [1156.5246    4.     2019.    ]\n",
      " [1133.0677    4.     2019.    ]\n",
      " [1098.2215    4.     2019.    ]\n",
      " [1078.8903    4.     2019.    ]]\n",
      "Instance loaded from 'out/post_train_markov_chain_model.pkl'.\n",
      "Starting testing\n",
      "Finished in 0.079 seconds.= -1.5270574125341428\n",
      "Sample index: <predicted load> vs. <actual load> (<percentage diff> %)\n",
      "0: 863.31 vs. 1093.88 (21.08 % diff)\n",
      "10: 1136.57 vs. 1381.37 (17.72 % diff)\n",
      "20: 997.67 vs. 1212.08 (17.69 % diff)\n",
      "30: 795.26 vs. 1116.67 (28.78 % diff)\n",
      "40: 975.94 vs. 1310.54 (25.53 % diff)\n",
      "50: 778.98 vs. 1059.19 (26.46 % diff)\n",
      "60: 1384.32 vs. 1622.43 (14.68 % diff)\n",
      "70: 1520.94 vs. 1156.15 (31.55 % diff)\n",
      "80: 1322.65 vs. 1618.47 (18.28 % diff)\n",
      "90: 1142.09 vs. 1388.30 (17.73 % diff)\n",
      "100: 808.53 vs. 1191.10 (32.12 % diff)\n",
      "110: 1257.33 vs. 1410.09 (10.83 % diff)\n",
      "120: 1445.70 vs. 1098.79 (31.57 % diff)\n",
      "130: 1144.48 vs. 1417.88 (19.28 % diff)\n",
      "140: 1513.79 vs. 1151.09 (31.51 % diff)\n",
      "150: 725.10 vs. 1005.77 (27.91 % diff)\n",
      "160: 822.47 vs. 1158.45 (29.00 % diff)\n",
      "MSE Markov Chain while predicting one sample ahead during training = 89265.93386360393.\n",
      "\n",
      "Markov Chain instance saved to 'out/post_test_markov_chain_model.pkl'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    # CONNECT TO DB, QUERY FOR DATA COLLECTION, CONVERT TO NUMPY\n",
    "    array = get_array_from_db()\n",
    "\n",
    "    # Print the type of 'array_without_outliers' and the top 5 samples for checking\n",
    "    print(\"Data as numpy array\")\n",
    "    print(f\"Type = {type(array)}\")\n",
    "    print(array[:5])\n",
    "    print()\n",
    "\n",
    "    # REMOVE OUTLIERS\n",
    "    # List of values for m\n",
    "    ms = [1, 2, 3, 4, 5, 6]\n",
    "    total_number_samples = plot_results_for_values_m('out/m_values', ms, array)\n",
    "\n",
    "    # Preferred value of m based on plot\n",
    "    preferred_m_value = 2\n",
    "    array_no_outliers = reject_outliers(array[:, 0], preferred_m_value)\n",
    "\n",
    "    new_sample_count = len(array_no_outliers)\n",
    "    count_diff = total_number_samples - new_sample_count\n",
    "    print(f\"Original sample count: {total_number_samples}\")\n",
    "    print(f\"Downsized to chosen  : {new_sample_count} ({count_diff} removed)\")\n",
    "    plot_hist_of_reduced('out/outliers_removed', total_number_samples, array_no_outliers)\n",
    "\n",
    "    # GET CHANGE THRESHOLDS FOR STATE TRANSITIONS OF MODEL\n",
    "    change_array = get_change_array(array_no_outliers)\n",
    "    print(f\"Load changes: shape = {change_array.shape}\")\n",
    "    print(change_array)\n",
    "    negative_changes, no_changes, positive_changes = get_changes_by_direction(change_array)\n",
    "    neg_min_value, neg_q1, neg_q2, neg_q3, neg_max_value, \\\n",
    "        pos_min_value, pos_q1, pos_q2, pos_q3, pos_max_value = \\\n",
    "        get_changes_five_summary_stats(negative_changes, positive_changes)\n",
    "    plot_change_summaries('out/changes', total_number_samples,\n",
    "                          negative_changes, no_changes, positive_changes,\n",
    "                          neg_min_value, neg_q1, neg_q2, neg_q3, neg_max_value,\n",
    "                          pos_min_value, pos_q1, pos_q2, pos_q3, pos_max_value)\n",
    "    print(f\"Negatives: Min = {neg_min_value:.5f}, Q1 = {neg_q1:.5f}, Q2 = {neg_q2:.5f}, \"\n",
    "          f\"Q3 = {neg_q3:.5f}, Max = {neg_max_value:.5f}\")\n",
    "    print(f\"No changes: count = {len(no_changes)}\")\n",
    "    print(f\"Positives: Min = {pos_min_value:.5f}, Q1 = {pos_q1:.5f}, Q2 = {pos_q2:.5f}, \"\n",
    "          f\"Q3 = {pos_q3:.5f}, Max = {pos_max_value:.5f}\")\n",
    "\n",
    "    # Number of change states (described above)\n",
    "    num_changes, num_months = 7, 12\n",
    "    targets = [neg_q1, neg_q2, neg_q3, 0, pos_q1, pos_q2, pos_q3]\n",
    "    print(f\"All targets:\\n{[round(target, 3) for target in targets]}\")\n",
    "\n",
    "    # OUTPUT FILE PATHS\n",
    "    train_out_path = 'out/train_preds_v_acts'\n",
    "    train_saved_model_path = 'out/post_train_markov_chain_model'\n",
    "    test_out_path = 'out/test_preds_v_acts'\n",
    "    test_saved_model_path = 'out/post_test_markov_chain_model'\n",
    "\n",
    "    # CREATE AND TRAIN THE MARKOV CHAIN\n",
    "    # Create an instance of Markov chain for the number of months and number of changes\n",
    "    markov_chain = MarkovChain(num_changes, num_months, targets)\n",
    "\n",
    "    # Start processing the data (array) and make a prediction for the next sample's load\n",
    "    print(f\"Starting training\")\n",
    "    start_time = time.time()\n",
    "    train_preds, train_acts = markov_chain.predict_and_train(train_out_path, array, prnt_every=0)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Finished in {execution_time:.3f} seconds.\")\n",
    "    plot_preds_versus_acts(train_out_path, array, train_preds, train_acts)\n",
    "    markov_chain.save_instance(train_saved_model_path)\n",
    "\n",
    "    # Print training results\n",
    "    train_preds, train_acts = load_results(train_out_path + \".txt\")\n",
    "    print_mse_and_results(train_preds, train_acts, 1000)\n",
    "\n",
    "    # TESTING\n",
    "    test_df = get_testing_df(\"./data/test_dataframes.xlsx\")\n",
    "    test_array = test_df.values\n",
    "    print(f\"{type(test_array)}: shape = {test_array.shape}\")\n",
    "    print(test_array[:5])\n",
    "    print(...)\n",
    "    print(test_array[-5:])\n",
    "\n",
    "    markov_chain = load_instance(train_saved_model_path)\n",
    "    print(f\"Starting testing\")\n",
    "    start_time = time.time()\n",
    "    test_preds, test_acts = markov_chain.predict_and_train(test_out_path, test_array, prnt_every=0)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Finished in {execution_time:.3f} seconds.\")\n",
    "\n",
    "    if test_preds is None:\n",
    "        test_preds, test_acts = load_results(test_out_path + \".txt\")\n",
    "    print_mse_and_results(test_preds, test_acts, 10)\n",
    "    markov_chain.save_instance(test_saved_model_path)\n",
    "\n",
    "\n",
    "def print_mse_and_results(preds, acts, print_every):\n",
    "    print(\"Sample index: <predicted load> vs. <actual load> (<percentage diff> %)\")\n",
    "    if print_every > 0:\n",
    "        for i, pred in enumerate(preds):\n",
    "            if i % print_every == 0:\n",
    "                print(f\"{i}: {pred:.2f} vs. {acts[i]:.2f} ({abs(pred - acts[i]) / acts[i] * 100:.2f} % diff)\")\n",
    "    mse_markov = mean_squared_error(acts, preds)\n",
    "    print(f\"MSE Markov Chain while predicting one sample ahead during training = {mse_markov}.\\n\")\n",
    "\n",
    "\n",
    "def get_array_from_db():\n",
    "    print(f\"Connecting to and retrieving data from the database.\")\n",
    "    start_time = time.time()\n",
    "    client, db, collection = init_db_connection()\n",
    "    mongo_data = retrieve_data(collection)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Finished in {execution_time:.3f} seconds.\")\n",
    "\n",
    "    pandas_df = convert_data_to_dataframe(mongo_data)\n",
    "    drop_dataframe_column('_id', pandas_df)\n",
    "    spark = init_spark_session()\n",
    "    spark_df = convert_pandas_df_to_spark(spark, pandas_df)\n",
    "    df_month_year = modify_spark_dataframe(spark_df)\n",
    "    array = convert_spark_df_to_numpy(df_month_year)\n",
    "    spark.stop()\n",
    "    return array\n",
    "\n",
    "\n",
    "def get_testing_df(file_path):\n",
    "    # Read the new data\n",
    "    df = pd.read_excel(file_path)\n",
    "    # Keep only the columns 'datetime' and 'nat_demand'\n",
    "    df = df[['datetime', 'DEMAND']]\n",
    "\n",
    "    # Make a copy of the dataframe\n",
    "    df_month_year = df.copy()\n",
    "\n",
    "    # Change column 'datetime' to type datetime\n",
    "    df_month_year['datetime'] = pd.to_datetime(df_month_year['datetime'])\n",
    "\n",
    "    # Replace datetime values with month numbers\n",
    "    df_month_year['month'] = df_month_year['datetime'].dt.month\n",
    "\n",
    "    # Add new column with the year\n",
    "    df_month_year['year'] = df_month_year['datetime'].dt.year\n",
    "\n",
    "    # Remove datetime column since no longer needed\n",
    "    return df_month_year.drop('datetime', axis=1)\n",
    "\n",
    "\n",
    "def load_results(save_path):\n",
    "    with open(save_path, 'r') as f:\n",
    "        json_str = f.read()\n",
    "    return json.loads(json_str)\n",
    "\n",
    "\n",
    "def init_db_connection():\n",
    "    # Initialize MongoDB Connection\n",
    "    client = MongoClient('mongodb+srv://JTXBigData:pJRAyKW9QnqE7B1G@jtxbigdatacluster.dzo50pn.mongodb.net/')\n",
    "    db = client['JTXBigDataCluster']\n",
    "    # collection = db['jtx-reduced-data']\n",
    "    collection = db['training-flattened']\n",
    "    return client, db, collection\n",
    "\n",
    "\n",
    "def retrieve_data(collection):\n",
    "    return list(collection.find())\n",
    "\n",
    "\n",
    "def convert_data_to_dataframe(mongo_data):\n",
    "    return pd.DataFrame(mongo_data)\n",
    "\n",
    "\n",
    "def drop_dataframe_column(name, pandas_df):\n",
    "    # Drop the _id column provided by MongoDB\n",
    "    if name in pandas_df.columns:\n",
    "        pandas_df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "def init_spark_session():\n",
    "    SparkSession.builder.config(\"spark.driver.host\", \"localhost\").getOrCreate()\n",
    "    return SparkSession.builder.appName(\"MongoDBToSparkDF\").getOrCreate()\n",
    "\n",
    "\n",
    "def convert_pandas_df_to_spark(spark, pandas_df):\n",
    "    return spark.createDataFrame(pandas_df)\n",
    "\n",
    "\n",
    "def modify_spark_dataframe(spark_df):\n",
    "    # Assuming spark_df is a Spark DataFrame\n",
    "    # Keep only the columns 'datetime' and 'DEMAND'\n",
    "    df = spark_df.select('datetime', 'DEMAND')\n",
    "\n",
    "    # Change column 'datetime' to type datetime\n",
    "    df = df.withColumn('datetime', df['datetime'].cast('timestamp'))\n",
    "\n",
    "    # Add new columns with month and year\n",
    "    df_month_year = df.withColumn('month', month('datetime')).withColumn('year', year('datetime'))\n",
    "\n",
    "    # Drop the 'datetime' column since it's no longer needed\n",
    "    df_month_year = df_month_year.drop('datetime')\n",
    "\n",
    "    return df_month_year\n",
    "\n",
    "\n",
    "# Function found online to remove outliers based on choice of m\n",
    "def reject_outliers(data, m):\n",
    "    return data[abs(data - np.mean(data)) < m * np.std(data)]\n",
    "\n",
    "\n",
    "def convert_spark_df_to_numpy(df_month_year):\n",
    "    # Convert Spark DataFrame to Pandas DataFrame\n",
    "    pandas_df_month_year = df_month_year.toPandas()\n",
    "\n",
    "    # Convert Pandas DataFrame to NumPy array\n",
    "    array = pandas_df_month_year.values\n",
    "    return array\n",
    "\n",
    "\n",
    "def plot_results_for_values_m(save_path, ms, array):\n",
    "    # Get total number of samples\n",
    "    total_number_samples = array.shape[0]\n",
    "\n",
    "    # Array of sample values for each different subset of the entire dataset\n",
    "    arrays_no_outliers = []\n",
    "\n",
    "    # Loop through different values of m\n",
    "    for m in ms:\n",
    "        # Use the reject_outliers function to remove outliers for the specified column ('DEMAND' in this case)\n",
    "        array_subset = reject_outliers(array[:, 0], m)\n",
    "        arrays_no_outliers.append(array_subset)\n",
    "        print(f\"m = {m}: {len(array_subset)} samples ({total_number_samples - len(array_subset)} \\\"outliers\\\" removed)\")\n",
    "    print()\n",
    "\n",
    "    # Boxplot with each sample subset distribution represented by a different box\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Apply labels on the x-axis for m values\n",
    "    box = ax.boxplot(arrays_no_outliers, patch_artist=True, labels=ms)\n",
    "\n",
    "    # Get unique color for each m value (in ms)\n",
    "    colors = distinctipy.get_colors(len(ms))\n",
    "\n",
    "    # Apply the colors to the boxplot boxes\n",
    "    for patch, color in zip(box['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "    # Label the plot\n",
    "    plt.title(\"Box plots of sample selections by values of m\")\n",
    "    plt.xlabel(\"M values\")\n",
    "    plt.ylabel(\"Load values\")\n",
    "    plt.savefig(save_path + \".png\", format='png')\n",
    "    # plt.show()\n",
    "    plt.clf()\n",
    "    return total_number_samples\n",
    "\n",
    "\n",
    "def plot_hist_of_reduced(save_path, total_number_samples, array_no_outliers):\n",
    "    num_bins = ceil(total_number_samples / 10)\n",
    "    plt.hist(array_no_outliers, bins=num_bins)\n",
    "    plt.title(\"Histogram of new sampling's load values\")\n",
    "    plt.xlabel(\"Load values\")\n",
    "    plt.ylabel(\"Number of samples\")\n",
    "    plt.savefig(save_path + \".png\", format='png')\n",
    "    # plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def get_change_array(array_no_outliers):\n",
    "    # Get the demand (load) as list for easy list comprehension\n",
    "    column_as_list = array_no_outliers.tolist()\n",
    "    total_number_samples = len(column_as_list)\n",
    "    # Create a new list of load changes between the next index and previous via list comprehension\n",
    "    change_list = [column_as_list[i + 1] - column_as_list[i] for i in range(total_number_samples - 1)]\n",
    "    return np.array(change_list)\n",
    "\n",
    "\n",
    "def get_changes_by_direction(change_array):\n",
    "    negative_changes = change_array[change_array < 0]\n",
    "    no_changes = change_array[change_array == 0]\n",
    "    positive_changes = change_array[change_array > 0]\n",
    "    return negative_changes, no_changes, positive_changes\n",
    "\n",
    "\n",
    "def get_changes_five_summary_stats(negative_changes, positive_changes):\n",
    "    # Get the min and max values for setting the plot x range\n",
    "    neg_min_value = np.min(negative_changes)\n",
    "    neg_max_value = np.max(negative_changes)\n",
    "\n",
    "    pos_min_value = np.min(positive_changes)\n",
    "    pos_max_value = np.max(positive_changes)\n",
    "\n",
    "    # Calculate Q1, Q2, Q3\n",
    "    neg_q1 = np.percentile(negative_changes, 25)\n",
    "    neg_q2 = np.percentile(negative_changes, 50)\n",
    "    neg_q3 = np.percentile(negative_changes, 75)\n",
    "\n",
    "    pos_q1 = np.percentile(positive_changes, 25)\n",
    "    pos_q2 = np.percentile(positive_changes, 50)\n",
    "    pos_q3 = np.percentile(positive_changes, 75)\n",
    "\n",
    "    return neg_min_value, neg_q1, neg_q2, neg_q3, neg_max_value, \\\n",
    "        pos_min_value, pos_q1, pos_q2, pos_q3, pos_max_value\n",
    "\n",
    "\n",
    "def plot_change_summaries(save_path, total_number_samples, negative_changes, no_changes, positive_changes,\n",
    "                          neg_min_value, neg_q1, neg_q2, neg_q3, neg_max_value,\n",
    "                          pos_min_value, pos_q1, pos_q2, pos_q3, pos_max_value):\n",
    "    # Set the number of bins for the histogram\n",
    "    num_bins = ceil(total_number_samples / 10)\n",
    "\n",
    "    # Create a histogram of the array\n",
    "    plt.hist(negative_changes, bins=num_bins, color='darkred', alpha=0.5, label='Negative Changes')\n",
    "    plt.hist(positive_changes, bins=num_bins, color='darkgreen', alpha=0.5, label='Positive Changes')\n",
    "    plt.hist(no_changes, bins=num_bins, color='gray', alpha=0.5, label='No Changes')\n",
    "\n",
    "    # Create vertical lines at each of Five-number summary\n",
    "    neg_line_color = 'orange'\n",
    "    plt.axvline(x=neg_min_value, color=neg_line_color)\n",
    "    plt.axvline(x=neg_q1, color=neg_line_color)\n",
    "    plt.axvline(x=neg_q2, color=neg_line_color)\n",
    "    plt.axvline(x=neg_q3, color=neg_line_color)\n",
    "    plt.axvline(x=neg_max_value, color=neg_line_color)\n",
    "\n",
    "    zero_line_color = 'gray'\n",
    "    plt.axvline(x=0, color=zero_line_color)\n",
    "\n",
    "    pos_line_color = 'turquoise'\n",
    "    plt.axvline(x=pos_min_value, color=pos_line_color)\n",
    "    plt.axvline(x=pos_q1, color=pos_line_color)\n",
    "    plt.axvline(x=pos_q2, color=pos_line_color)\n",
    "    plt.axvline(x=pos_q3, color=pos_line_color)\n",
    "    plt.axvline(x=pos_max_value, color=pos_line_color)\n",
    "\n",
    "    # Label plot\n",
    "    plt.title(\"Histogram of all load changes\")\n",
    "    plt.xlabel(\"Load change values\")\n",
    "    plt.ylabel(\"Number of samples\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(save_path + \".png\", format='png')\n",
    "    # plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "class State:\n",
    "    times_in_state = 0  # Number of times the model has been in this state\n",
    "    total_load = 0  # The running load of all time state is visited\n",
    "    prev_load = 0  # The last seen load value\n",
    "\n",
    "    # Class constructor\n",
    "    def __init__(self, i, j, num_changes, targets):\n",
    "        self.month = i\n",
    "        self.change = j\n",
    "        self.same_month_trans_counts = np.zeros(num_changes)  # transition probabilities for same month changes\n",
    "        self.next_month_trans_counts = np.zeros(num_changes)  # transition probabilities for next month changes\n",
    "        self.targets = targets\n",
    "\n",
    "    # Currently went to this node based on training observation\n",
    "    def visit(self, load):\n",
    "        self.times_in_state += 1\n",
    "        self.total_load += load\n",
    "        self.prev_load = load\n",
    "\n",
    "    # Used for predicting load from the previous state to this one\n",
    "    def get_load(self, cur_load):\n",
    "        return cur_load + self.targets[self.change]\n",
    "\n",
    "    # Function to print out Node object instance info for checking\n",
    "    def print_out(self):\n",
    "        if self.times_in_state > 0:\n",
    "            avg = self.total_load / self.times_in_state\n",
    "        else:\n",
    "            avg = self.prev_load\n",
    "        print(f\"State: ({self.month}, {self.change}), Visits: {self.times_in_state}, \"\n",
    "              f\"Prev load (avg): {self.prev_load} ({avg})\\n\"\n",
    "              f\"\\tMonth {self.month} transitions: {self.same_month_trans_counts}\\n\"\n",
    "              f\"\\tMonth {(self.month + 1) % 12} transitions: {self.next_month_trans_counts}\"\n",
    "              )\n",
    "\n",
    "    # Returns the top transition change pick based on the current stochastic model\n",
    "    def get_state_transition_pred(self, same_month):\n",
    "\n",
    "        # Month is staying the same\n",
    "        if same_month:\n",
    "            trans_counts = self.same_month_trans_counts\n",
    "\n",
    "        # Month is moving to the next month\n",
    "        else:\n",
    "            trans_counts = self.next_month_trans_counts\n",
    "\n",
    "        # Get the index of the top pick (largest number of times moved to that change from this one)\n",
    "        top_change_pick = np.argmax(trans_counts)\n",
    "\n",
    "        # If been in this state at least once\n",
    "        if self.times_in_state > 0:\n",
    "\n",
    "            # Will have confidence above indifferent (1/num_changes%)\n",
    "            prob_top_pick = np.max(trans_counts) / self.times_in_state\n",
    "\n",
    "        else:\n",
    "            # Confidence is indifferent (same for all possible changes)\n",
    "            prob_top_pick = 1 / len(self.same_month_trans_counts)\n",
    "\n",
    "        # Return the top change pick and associated confidence\n",
    "        return top_change_pick, prob_top_pick\n",
    "\n",
    "\n",
    "# Class for the Markov Chain model\n",
    "class MarkovChain:\n",
    "\n",
    "    # Constructor initializes the needed transition matrix automatically\n",
    "    def __init__(self, num_changes, num_months, targets):\n",
    "        self.num_changes = num_changes\n",
    "        self.num_months = num_months\n",
    "        self.targets = targets\n",
    "        self._init_transition_matrix()\n",
    "\n",
    "    # Initializes a transition matrix based on constructor parameters\n",
    "    def _init_transition_matrix(self):\n",
    "        # Create empty array of dimensions number_months by num_changes (12 by 7) to store State object references\n",
    "        transition_matrix = np.empty((self.num_months, self.num_changes), dtype=State)\n",
    "        # for each month [0-11]\n",
    "        for i in range(self.num_months):\n",
    "            # For each change [0-6]\n",
    "            for j in range(self.num_changes):\n",
    "                transition_matrix[i][j] = State(int(i), j, self.num_changes, self.targets)\n",
    "        # Initialize instance variable to created matrix\n",
    "        self.transition_matrix = transition_matrix\n",
    "\n",
    "    # Method to iterate over data samples and make predictions while updating the model to keep track of prior\n",
    "    # observations/state transitions\n",
    "    def predict_and_train(self, save_path, sample_set, prnt_every=10000):\n",
    "\n",
    "        # Get the starting month\n",
    "        starting_month = int(sample_set[0, 1])\n",
    "\n",
    "        # Set starting change to NO CHANGE (since no previous information)\n",
    "        starting_change = 3\n",
    "\n",
    "        # Get starting state from transition matrix using key scheme\n",
    "        starting_state = self.transition_matrix[starting_month, starting_change]\n",
    "\n",
    "        # Get starting load from first sample\n",
    "        starting_load = sample_set[0, 0]\n",
    "\n",
    "        # Visit the starting state passing cur load\n",
    "        starting_state.visit(starting_load)\n",
    "\n",
    "        # Set prev state as the starting\n",
    "        prev_state = starting_state\n",
    "\n",
    "        # set up lists to add predictions to\n",
    "        preds, trues = [], []\n",
    "\n",
    "        # For each sample in training set except the first (used to seed start)\n",
    "        for i, sample in enumerate(sample_set[1:]):  # start at second training sample\n",
    "\n",
    "            # Offset i to reflect skipping 1\n",
    "            i += 1\n",
    "\n",
    "            # Get the cur and prev month and load values\n",
    "            cur_month = int(sample[1])\n",
    "            cur_load = sample[0]\n",
    "            prev_month = prev_state.month\n",
    "            prev_load = prev_state.prev_load\n",
    "\n",
    "            # Compute load change\n",
    "            load_change = cur_load - prev_load\n",
    "\n",
    "            # Determine which change the current change is closest to target wise\n",
    "            closest_target = min(self.targets, key=lambda x: abs(x - load_change))\n",
    "\n",
    "            # Set change transition target to index of the closest value\n",
    "            change_target = self.targets.index(closest_target)\n",
    "\n",
    "            # Boolean if month is same between prev and cur\n",
    "            same_month = False\n",
    "\n",
    "            # Ensure cur_month is within the valid range [0, 11]\n",
    "            cur_month = cur_month % self.num_months\n",
    "\n",
    "            # CHeck if staying in the same month\n",
    "            if cur_month == prev_month:\n",
    "\n",
    "                same_month = True\n",
    "                # Increment change target tally of the state\n",
    "                prev_state.same_month_trans_counts[change_target] += 1\n",
    "\n",
    "            # Later month\n",
    "            elif cur_month != prev_month:\n",
    "                # Initialize a flag to check for a valid month transition\n",
    "                valid_transition = False\n",
    "    \n",
    "                # Check for valid month transitions [0-11]\n",
    "                for month_offset in range(1, self.num_months):\n",
    "                    if cur_month == (prev_month + month_offset) % self.num_months:\n",
    "                       valid_transition = True\n",
    "                       break\n",
    "    \n",
    "                # Check if a valid month transition was found\n",
    "                if valid_transition:\n",
    "                   # Increment change target tally of the state \n",
    "                   prev_state.next_month_trans_counts[change_target] += 1\n",
    "                else:\n",
    "                   # Stop execution because training will be invalid\n",
    "                   error_msg = f\"Encountered an invalid forward state transition from {prev_month} -> {cur_month}.\"\n",
    "                   print(error_msg)  \n",
    "                   print(f\"cur_month: {cur_month}, prev_month: {prev_month}\")  \n",
    "                   raise ValueError(error_msg)\n",
    "\n",
    "\n",
    "            # Get predicted next change and associated probability (confidence)\n",
    "            predicted_next_change, prob = prev_state.get_state_transition_pred(same_month)\n",
    "\n",
    "            # Get the target month (month of next sample to predict), same or next\n",
    "            next_index = (i + 1) % len(sample_set)\n",
    "            next_sample_month = int(sample_set[next_index][1])\n",
    "\n",
    "            # Get the predicted next state using predicted change and known target month\n",
    "            # predicted_next_state = self.transition_matrix[next_sample_month, predicted_next_change]\n",
    "            predicted_next_state = self.transition_matrix[next_sample_month - 1, predicted_next_change]\n",
    "\n",
    "            # Get prediction for next load value\n",
    "            predicted_next_load = predicted_next_state.get_load(cur_load)\n",
    "\n",
    "            # Add prediction to the running predictions list\n",
    "            preds.append(predicted_next_load)\n",
    "\n",
    "            # Get the actual next load\n",
    "            actual_next_load = sample_set[next_index][0]\n",
    "\n",
    "            # Add the actual next load to actuals running list\n",
    "            trues.append(actual_next_load)\n",
    "\n",
    "            # Compute the percentage difference prediction is from the \"true\" value\n",
    "            predict_diff = abs((actual_next_load - predicted_next_load) / actual_next_load * 100)\n",
    "\n",
    "            # Calculate R-squared\n",
    "            if len(preds) > 1:\n",
    "                r2 = r2_score(trues, preds)\n",
    "\n",
    "            # Metric is considered undefined for single pair\n",
    "            else:\n",
    "                r2 = \"undefined\"\n",
    "\n",
    "            # Get the state that reflects current load change\n",
    "            cur_state = self.transition_matrix[cur_month, change_target]\n",
    "            cur_state.visit(cur_load)\n",
    "\n",
    "            # Print out extra info by specified iteration cycle for checking\n",
    "            if prnt_every > 0 and i % prnt_every == 0:\n",
    "                print(f\"i = {i}/{len(sample_set) - 1}\")\n",
    "                print(f\"Month: ({prev_month} -> {cur_month}), Load: ({prev_load} -> {cur_load})\")\n",
    "                print(f\"Load change: {load_change} -> {closest_target}, Change type: {change_target}\")\n",
    "                print(\"Prev state:\")\n",
    "                print(f\"   Same month transitions: {prev_state.same_month_trans_counts}\")\n",
    "                print(f\"   Next month transitions: {prev_state.next_month_trans_counts}\")\n",
    "\n",
    "                print(\"Cur state:\")\n",
    "                print(f\"   Same month transitions: {cur_state.same_month_trans_counts}\")\n",
    "                print(f\"   Next month transitions: {cur_state.next_month_trans_counts}\")\n",
    "                print(f\"Predicted change: {predicted_next_change}, Actual change: {change_target}\")\n",
    "                print(\n",
    "                    f\"Prediction: {predicted_next_load:.2f}, Actual: {actual_next_load:.2f}, \"\n",
    "                    f\"% Difference: {predict_diff:.2f} %\")\n",
    "                print(\"-----------------------------------------------------------------------------\\n\")\n",
    "\n",
    "                # Print out current iteration, relative progress percentage and running r-square measure across the\n",
    "                # entire set\n",
    "            print(f\"i = {i}/{len(sample_set) - 1} ({i / len(sample_set) * 100:.2f} %): r2 = {r2}\", end=\"\\r\")\n",
    "\n",
    "            # Set previous state to current state for next iteration\n",
    "            prev_state = cur_state\n",
    "\n",
    "        results_tuple = (preds, trues)\n",
    "        # Convert the tuple to a JSON string\n",
    "        json_str = json.dumps(results_tuple)\n",
    "        # Save the JSON string to a file\n",
    "        with open(save_path + \".txt\", 'w') as f:\n",
    "            f.write(json_str)\n",
    "\n",
    "        # once all samples have been processed, return predictions and associated true load values\n",
    "        return preds, trues\n",
    "\n",
    "    def save_instance(self, save_path):\n",
    "        # Save the instance to a file\n",
    "        file_path = save_path + '.pkl'\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        print(f\"Markov Chain instance saved to '{file_path}'.\")\n",
    "\n",
    "\n",
    "def load_instance(load_path):\n",
    "    file_path = load_path + '.pkl'\n",
    "    with open(file_path, 'rb') as f:\n",
    "        instance = pickle.load(f)\n",
    "    print(f\"Instance loaded from '{file_path}'.\")\n",
    "    return instance\n",
    "\n",
    "\n",
    "# Function to plot the preds vs actual load values\n",
    "def plot_preds_versus_acts(save_path, array, preds, acts):\n",
    "    # Get unique years in the dataset and count per year\n",
    "    years, year_counts = np.unique(array[:, 2], return_counts=True)\n",
    "\n",
    "    print(f\"Years: {years}\")\n",
    "    print(f\"Counts: {year_counts}\")\n",
    "\n",
    "    # generate visually distinct colors for each year\n",
    "    colors = distinctipy.get_colors(len(years))\n",
    "    line_width = 0.5\n",
    "    plt.plot(preds, label='Predictions', linewidth=line_width)\n",
    "    plt.plot(acts, label='Actual', linewidth=line_width)\n",
    "    plt.legend()\n",
    "    plt.title(\"Predictions vs. Actual load values\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Load value\")\n",
    "\n",
    "    # Place vertical lines between each sample group of same year\n",
    "    index = 0\n",
    "    for i, y in enumerate(years):\n",
    "        index += year_counts[i]\n",
    "        plt.axvline(x=index, color=colors[i], label=y)\n",
    "    plt.savefig(save_path + \".png\", format='png')\n",
    "    # plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
